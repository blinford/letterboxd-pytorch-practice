{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21b5cc46-0b06-4a20-8637-adc99ede78ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>film</th>\n",
       "      <th>rating</th>\n",
       "      <th>is_five_star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>an-ostrich-told-me-the-world-is-fake-and-i-thi...</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gravity-2013</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>black-narcissus</td>\n",
       "      <td>★★★★</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>avengers-infinity-war</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the-apartment</td>\n",
       "      <td>★★★★½</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>zathura-a-space-adventure</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>the-tree-of-life-2011</td>\n",
       "      <td>★★★★½</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>inland-empire</td>\n",
       "      <td>★★★★★</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>drive-my-car</td>\n",
       "      <td>★★★★★</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>david-lynch-cooks-quinoa</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>589 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  film rating  is_five_star\n",
       "0    an-ostrich-told-me-the-world-is-fake-and-i-thi...                False\n",
       "1                                         gravity-2013                False\n",
       "2                                      black-narcissus   ★★★★         False\n",
       "3                                avengers-infinity-war                False\n",
       "4                                        the-apartment  ★★★★½         False\n",
       "..                                                 ...    ...           ...\n",
       "584                          zathura-a-space-adventure                False\n",
       "585                              the-tree-of-life-2011  ★★★★½         False\n",
       "586                                      inland-empire  ★★★★★          True\n",
       "587                                       drive-my-car  ★★★★★          True\n",
       "588                           david-lynch-cooks-quinoa                False\n",
       "\n",
       "[589 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# personal ratings json file from letterboxd\n",
    "with open('response_1743040116816.json', 'r', encoding='utf-8') as file:\n",
    "    ratings_df = pd.DataFrame(json.load(file)['films'])\n",
    "ratings_df['is_five_star'] = ratings_df['rating'].apply(lambda x: x=='★★★★★')\n",
    "ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1426521-8321-4a52-9bba-66f78b59e4cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# url of api hosted on cloudrun\n",
    "letterboxd_api_url = 'https://letterboxd-api-....us-east1.run.app/film/{film}/reviews'\n",
    "\n",
    "def get_reviews(film):\n",
    "  # print(letterboxd_api_url.format(film=film))\n",
    "  reviews = requests.get(letterboxd_api_url.format(film=film), allow_redirects=True).text\n",
    "  return reviews\n",
    "\n",
    "async def get_reviews_async(df, film):\n",
    "  async with aiohttp.ClientSession() as session:\n",
    "    resp = await session.get(letterboxd_api_url.format(film=film), allow_redirects=True)\n",
    "    df.loc[df['film'] == film, 'reviews'] = resp\n",
    "\n",
    "# await asyncio.gather(*[get_reviews_async(ratings_df, film) for film in ratings_df['film']])\n",
    "ratings_df['reviews'] = ratings_df['film'].apply(get_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69fff966-0f79-4976-91f4-64d549356296",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.to_csv('ratings-with-reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7e8e2a4-bb52-40cd-ae5b-52ea793a1c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved to file and reloaded to prevent any data loss\n",
    "ratings_df = pd.read_csv('ratings-with-reviews.csv')\n",
    "ratings_df['response'] = ratings_df['reviews']\n",
    "\n",
    "def get_reviews(x):\n",
    "    try:\n",
    "        return json.loads(x)['reviews']\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "ratings_df['reviews'] = ratings_df['reviews'].apply(get_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "609468bb-05fe-4a81-85cf-9e2e0b99dafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49239d75-bb69-4d5c-bb19-23f1872aa7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LongformerTokenizer, LongformerModel\n",
    "\n",
    "model_name = \"allenai/longformer-base-4096\" \n",
    "tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
    "model = LongformerModel.from_pretrained(model_name).cuda()\n",
    "\n",
    "def get_embeddings(reviews):\n",
    "    input_ids = tokenizer('<sep>'.join(reviews[:10]), return_tensors=\"pt\", padding=True).input_ids.cuda()\n",
    "    with torch.no_grad():\n",
    "      return model(input_ids).last_hidden_state\n",
    "        \n",
    "ratings_df['lf_embeddings'] = ratings_df['reviews'].dropna().apply(get_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe83c759-ceab-48f8-a4c7-a2d157b9e40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternate approach: encode each review as a sentence then take avg/max of the embedding element wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c27103-3b26-415c-94ad-568237abd7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers -q\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "62383d09-0c08-46b1-bb21-e2274ce14efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_reviews(reviews):\n",
    "    return sentence_model.encode(reviews[:10])\n",
    "\n",
    "ratings_df['embeddings'] = ratings_df['reviews'].dropna().apply(encode_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3bf1e886-72d5-492d-a1af-c6dd0f28521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ratings_df['avg_embedding'] = ratings_df['embeddings'].dropna().apply(lambda x: np.mean(x, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2551e81d-598d-485d-9da6-cc5605c2c9f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ratings_df['embedding'] = ratings_df['avg_embedding'].dropna().apply(lambda x: np.array(x))\n",
    "s = ratings_df['embedding'].apply(lambda x: not isinstance(x, float))\n",
    "ratings_df = ratings_df[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af36dda1-26e1-42bf-98be-f67502397e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(ratings_df[['lf_embeddings', 'is_five_star', 'film']], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eadc1e-3637-44e1-86d2-bb792911b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.embeddings = torch.tensor(df['lf_embeddings'].to_list()).float()\n",
    "        def helper(val):\n",
    "            return [1, 0] if val else [0, 1]\n",
    "        self.ratings = torch.tensor(ratings_df['is_five_star'].apply(helper).to_list()).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.ratings[idx]\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = MovieDataset(train)\n",
    "test_dataset = MovieDataset(test)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef378283-3fee-4ff5-8cf5-fc7bed1f23b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(torch.tensor(ratings_df['embedding'].to_list()).float())\n",
    "torch.tensor(ratings_df['is_five_star'].apply(lambda x: [x]).to_list()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "735525fd-f3c7-4d3f-9dc4-39a4d4a689e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork(sentence_model.get_sentence_embedding_dimension()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "cf4ea799-e5bd-43c2-868b-7867d91cefc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# use weightings to \n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([20, 1]).cuda())\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "44059243-5e58-4ff2-a692-6abeb9fdfde4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], loss: 1.0210\n",
      "epoch [2/100], loss: 1.6913\n",
      "epoch [3/100], loss: 1.6805\n",
      "epoch [4/100], loss: 2.3304\n",
      "epoch [5/100], loss: 1.3322\n",
      "epoch [6/100], loss: 1.0013\n",
      "epoch [7/100], loss: 0.9947\n",
      "epoch [8/100], loss: 2.2623\n",
      "epoch [9/100], loss: 0.6691\n",
      "epoch [10/100], loss: 0.9742\n",
      "epoch [11/100], loss: 0.6604\n",
      "epoch [12/100], loss: 1.2614\n",
      "epoch [13/100], loss: 1.2585\n",
      "epoch [14/100], loss: 0.6493\n",
      "epoch [15/100], loss: 0.6412\n",
      "epoch [16/100], loss: 0.6347\n",
      "epoch [17/100], loss: 0.6289\n",
      "epoch [18/100], loss: 1.4501\n",
      "epoch [19/100], loss: 1.1735\n",
      "epoch [20/100], loss: 1.1395\n",
      "epoch [21/100], loss: 0.6163\n",
      "epoch [22/100], loss: 0.8597\n",
      "epoch [23/100], loss: 0.8650\n",
      "epoch [24/100], loss: 1.3509\n",
      "epoch [25/100], loss: 1.8662\n",
      "epoch [26/100], loss: 0.8409\n",
      "epoch [27/100], loss: 1.3325\n",
      "epoch [28/100], loss: 0.6060\n",
      "epoch [29/100], loss: 0.8336\n",
      "epoch [30/100], loss: 0.8319\n",
      "epoch [31/100], loss: 1.5802\n",
      "epoch [32/100], loss: 1.0709\n",
      "epoch [33/100], loss: 1.3166\n",
      "epoch [34/100], loss: 0.5884\n",
      "epoch [35/100], loss: 1.0690\n",
      "epoch [36/100], loss: 0.8269\n",
      "epoch [37/100], loss: 1.3407\n",
      "epoch [38/100], loss: 0.5699\n",
      "epoch [39/100], loss: 0.5659\n",
      "epoch [40/100], loss: 1.0903\n",
      "epoch [41/100], loss: 0.7804\n",
      "epoch [42/100], loss: 0.5389\n",
      "epoch [43/100], loss: 1.0756\n",
      "epoch [44/100], loss: 0.7940\n",
      "epoch [45/100], loss: 1.6163\n",
      "epoch [46/100], loss: 0.7880\n",
      "epoch [47/100], loss: 0.5570\n",
      "epoch [48/100], loss: 0.7884\n",
      "epoch [49/100], loss: 1.0426\n",
      "epoch [50/100], loss: 0.7937\n",
      "epoch [51/100], loss: 0.5436\n",
      "epoch [52/100], loss: 0.8187\n",
      "epoch [53/100], loss: 1.0473\n",
      "epoch [54/100], loss: 0.5000\n",
      "epoch [55/100], loss: 0.7462\n",
      "epoch [56/100], loss: 1.3263\n",
      "epoch [57/100], loss: 0.9956\n",
      "epoch [58/100], loss: 1.2607\n",
      "epoch [59/100], loss: 0.5093\n",
      "epoch [60/100], loss: 0.5098\n",
      "epoch [61/100], loss: 0.8252\n",
      "epoch [62/100], loss: 1.0189\n",
      "epoch [63/100], loss: 1.0168\n",
      "epoch [64/100], loss: 1.3161\n",
      "epoch [65/100], loss: 1.0389\n",
      "epoch [66/100], loss: 0.7667\n",
      "epoch [67/100], loss: 1.2243\n",
      "epoch [68/100], loss: 1.0052\n",
      "epoch [69/100], loss: 0.9509\n",
      "epoch [70/100], loss: 1.1118\n",
      "epoch [71/100], loss: 1.1884\n",
      "epoch [72/100], loss: 0.7844\n",
      "epoch [73/100], loss: 0.6913\n",
      "epoch [74/100], loss: 0.8241\n",
      "epoch [75/100], loss: 1.1445\n",
      "epoch [76/100], loss: 1.1643\n",
      "epoch [77/100], loss: 0.6954\n",
      "epoch [78/100], loss: 0.6186\n",
      "epoch [79/100], loss: 0.6596\n",
      "epoch [80/100], loss: 0.6092\n",
      "epoch [81/100], loss: 0.6676\n",
      "epoch [82/100], loss: 0.4449\n",
      "epoch [83/100], loss: 0.6770\n",
      "epoch [84/100], loss: 1.5231\n",
      "epoch [85/100], loss: 0.4138\n",
      "epoch [86/100], loss: 0.4655\n",
      "epoch [87/100], loss: 0.6238\n",
      "epoch [88/100], loss: 0.3416\n",
      "epoch [89/100], loss: 0.7383\n",
      "epoch [90/100], loss: 0.5911\n",
      "epoch [91/100], loss: 0.3647\n",
      "epoch [92/100], loss: 1.2818\n",
      "epoch [93/100], loss: 0.3329\n",
      "epoch [94/100], loss: 1.2055\n",
      "epoch [95/100], loss: 0.5891\n",
      "epoch [96/100], loss: 0.8282\n",
      "epoch [97/100], loss: 0.5087\n",
      "epoch [98/100], loss: 1.0159\n",
      "epoch [99/100], loss: 0.4109\n",
      "epoch [100/100], loss: 0.8547\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_loader:\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            model.cuda()\n",
    "        else:\n",
    "            model.cpu()\n",
    "            \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'epoch [{epoch+1}/{num_epochs}], loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66d0919-00ac-4677-a6a3-120db88b2a14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            model.cuda()\n",
    "        else:\n",
    "            model.cpu()\n",
    "            \n",
    "        outputs = model(inputs)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        print(outputs)\n",
    "        print(predicted)\n",
    "        print(f'predicted: {predicted.item()}, actual: {targets.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce44052e-61f1-4684-b3ce-98e92f180fb9",
   "metadata": {},
   "source": [
    "findings\n",
    "\n",
    "tried two main tasks both based on reviews left by other letterboxd users as input and my rating as the label:\n",
    "1. linear regression to predict score, but the average prediction was equal to the actual average of my ratings (the model rated almost all films around 4.25, with very little variance)\n",
    "2. since that didn't work out great, I decided to try binary classification, predicting whether I rated a movie five stars, using BCE with logits loss, weighted because there were many more non five star ratings than five star--still not great, the model struggled to converge anywhere\n",
    "\n",
    "this project will need a lot more work before coming up with anything great, honestly it's tricky to tell what my five star ratinsg would be based on the reviews. I should find a task more suited toward the data, maybe move away from simple regression/classification problems and toward more specific recommender systems--would probably have more use for me anyway"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
